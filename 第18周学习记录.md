### Fun Facts

这次的代码运行明显慢了一些，风扇也开始转了。

<img src="C:\Users\Muschuang123\AppData\Roaming\Typora\typora-user-images\image-20241228103148939.png" alt="image-20241228103148939" style="zoom:50%;" />

## softmax的从零开始实现

这里先放一张上次学习看到的图，我觉得这张图非常好的表示出了实现softmax回归的过程：

<img src="C:\Users\Muschuang123\AppData\Roaming\Typora\typora-user-images\image-20241228105235193.png" alt="image-20241228105235193" style="zoom:50%;" />

### 初始化模型参数

这次不是自己随机构造数据集了，用于训练的数据集是**Fashion MNIST**数据集，通过我查找资料，发现这是一个经典的数据集，主要适用于初学者学习。这个数据集中的每个数据点都是一个**28\*28**的灰度图像，而我们需要将这些图像分出**10类**（0~9），这也是下面的`num_inputs`是784（28×28）、`num_outputs`是10的原因。

```Python
num_inputs = 784
num_outputs = 10

W = torch.normal(0, 0.01, size=(num_inputs, num_outputs), requires_grad=True)
b = torch.zeros(num_outputs, requires_grad=True)
```

1. 与线性回归不同的是，这次的`W`是一个矩阵，而不再是一个向量。
2. 这个`W`和`b`在现在都是随机的，之后我们要通过训练来改变`W`和`b`的值。

### 定义softmax的操作：

回顾上次学习的内容（取对数：允许节点值为负）：

```python
def softmax(X):
    X_exp = torch.exp(X)
    partition = X_exp.sum(1, keepdim=True)
    return X_exp / partition  # 这里应用了广播机制
```

这段代码可以将传入的`X`（等同于上图中的n个`z`组成的矩阵）转成softmax函数值。具体地，先使用`torch.exp()`函数给每个值取指数，然后在用计算`partition`（每一行的指数和），然后利用广播机制让每个值都除以相应的行对应的指数和。

#### 上溢和下溢

文中说了这样实现不是准确的。因为如果`X`中有元素过大或者过小的值会导致**上溢**或**下溢**。主要原因是浮点型的特性造成的。**上溢**是因为**e^x^**可能过大，浮点型精度不够表示不了。**下溢**是因为**x**是负数并且太小，浮点型精度不够直接变成0了。这在分子上不要紧，但是在分母上就是除0错误。

不过解决方案也不难，就是上下同除**max{x}**。这样原本的上溢情况下，分子变成了1；下溢情况下，分母起码有一个1值。

虽然但是，这组数据集应该不会造成上溢或者下溢吧。

### 定义模型

可以再看一下一开始放的那张图。就能很快理解这段代码：

```python
def net(X):
    return softmax(torch.matmul(X.reshape((-1, W.shape[0])), W) + b)
```

`X`就是输入，这里把输入先扔进**决策函数**（就是`W`和`b`的那个），每个值对应的决策函数值都能算出来。算出来之后再代入softmax函数，算出值。

### 损失函数

再来一张图：

<img src="C:\Users\Muschuang123\AppData\Roaming\Typora\typora-user-images\image-20241228112253934.png" alt="image-20241228112253934" style="zoom: 50%;" />

这个损失函数作用就是把**预测值**跟**真实值**比对（其他的值都不看，只看独热编码中的**1**在哪里）。然后计算出个函数值，其实我们并不关心这个函数值具体是多少，我们只需要用它来反向传播训练`W`和`b`就可以了。我认为这个训练其实本质上是训练决策函数，让决策函数的预测结果更精准。

图中的这个方程只是针对单个样本的，计算多个样本的话取个mean就可以了。

虽然这个损失函数在数学上写起来比较复杂，但是在Python中实现非常简单，例如：

```python
y = torch.tensor([0, 2])
y_hat = torch.tensor([[0.1, 0.3, 0.6], [0.3, 0.2, 0.5]])
y_hat[[0, 1], y]
```

我来讲讲这段代码什么意思：

`y_hat`就是一个**预测向量**，然后`y`代表这个**每个预测值**的**正确预测**是哪个。本质上就是一个下标的选择。

对于`y_hat`的第`[0, 1]`个向量，选择他们的第`[0, 2]`个值。输出结果：

```
tensor([0.1000, 0.5000])
```

所以损失函数就可以这么写了：

```python
def cross_entropy(y_hat, y):
    return - torch.log(y_hat[range(len(y_hat)), y])

cross_entropy(y_hat, y)
```

虽然听不懂什么是交叉熵，但是把这个损失函数弄懂了。（）

<img src="C:\Users\Muschuang123\AppData\Roaming\Typora\typora-user-images\image-20241228114144265.png" alt="image-20241228114144265" style="zoom: 67%;" />

### 分类精度

读了书上的文字，感觉**分类精度**就是**预测是否正确**的精度。这个预测就是必须要给出答案了，对于每个样本，我们必须要告诉评测数据我选的啥。选对了就对（1），选错了就错（0）。然后用**正确的预测数量/总样本数量**得到**分类精度率**。

```python
def accuracy(y_hat, y):  #@save
    """计算预测正确的数量"""
    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:
        y_hat = y_hat.argmax(axis=1)
    cmp = y_hat.type(y.dtype) == y
    return float(cmp.type(y.dtype).sum())
```

求分类精度率的话就`accuracy(y_hat, y) / len(y)`。

1. 这里`if`语句判断预测结果有`> 1`个**维度**，并且**第二个维度上的元素个数**也`> 1`。

2. 关于**`argmax()`**，其实网上说了一大堆，不就是个求最大值点的函数吗。。这里一算就算出预测结果了。

3. `==`对元素类型非常敏感，所以这里需要验证一下`y_hat`的类型。对于**`type()`**，

   > 该方法的功能是: 当不指定dtype时,返回类型. 当指定dtype时,返回类型转换后的数据,如果类型已经符合要求, 那么不做额外的复制,返回原对象.

4. 返回`float`类型是为了方便计算精度分辨率？不过对于python，`/`默认不就是带小数的计算吗，这里没看懂。我猜测是为了保持张量类型的统一，因为`y`和`y_hat`都是float类型的。

```python
def evaluate_accuracy(net, data_iter):  #@save
    """计算在指定数据集上模型的精度"""
    if isinstance(net, torch.nn.Module):
        net.eval()  # 将模型设置为评估模式
    metric = Accumulator(2)  # 正确预测数、预测总数
    with torch.no_grad():
        for X, y in data_iter:
            metric.add(accuracy(net(X), y), y.numel())
    return metric[0] / metric[1]
```

1. `net.eval()`和`torch.no_grad()`有同一个目的。因为这是在评估，所以不用加跟踪梯度之类的训练操作，可以减少内存消耗。

2. `metric`用一个`Accumulator`类（他自己定义的）初始化，这个类可以求和。用这个方便地算出**正确的预测数量/总样本数量**。

   > ```python
   > class Accumulator:  #@save
   >     """在n个变量上累加"""
   >     def __init__(self, n):
   >         self.data = [0.0] * n
   > 
   >     def add(self, *args):
   >         self.data = [a + float(b) for a, b in zip(self.data, args)]
   > 
   >     def reset(self):
   >         self.data = [0.0] * len(self.data)
   > 
   >     def __getitem__(self, idx):
   >         return self.data[idx]
   > ```

3. 这个`if isinstance()`可以判断两个参数的类型是否相同，如果传进来的`net`不是`torch.nn.Module`的话就不设置为评估模式（应该是只有`torch.nn.Module`才有评估模式。

4. 按照我们随机初始化的`W`和`b`，代入`evaluate_accuracy()`得到的值肯定很小，因为这基本就是在猜。（从零开始（迫真

### 训练

这一块的代码 很长，我看了一下。基本跟线性回归那块思路差不多，因为训练模型和损失函数都定义好了，这一块就是把数据往里带，然后反向传播更新模型。书里还加了一个**在动画中绘制数据的类**，这样我们就可以看到训练的过程了。具体可以见本次学习记录开头。

<img src="C:\Users\Muschuang123\AppData\Roaming\Typora\typora-user-images\image-20241228123332974.png" alt="image-20241228123332974" style="zoom:50%;" />

这是训练好了，可以成功预测结果的状态。

## 总结

这次其实就上把上次关于softmax回归的概念内容落实到程序上了，我也确实看到了真模型，跑了真代码。不过这么点的数据量，这么大小的图片就得跑1分钟，那如果数据大了，跑的时间可能真的很长，需要比较高的算力。不过对于简单练习的话，笔记本随便跑跑还是能跑出来的。

## 参考文献 / 学习资料

1. [《动手学深度学习》 — 动手学深度学习 2.0.0 documentation](https://zh-v2.d2l.ai/)
2. [【深度学习 搞笑教程】16 softmax回归 | 草履虫都能听懂 零基础入门 | 持续更新](https://www.bilibili.com/video/BV1Cw411C71D/)
3. [Python isinstance() 函数 | 菜鸟教程](https://www.runoob.com/python/python-func-isinstance.html)
4. [Pytorch中的Net.train()和 Net.eval()函数讲解-CSDN博客](https://blog.csdn.net/weixin_47872288/article/details/134690413)
5. [torch.Tensor.type()使用举例-CSDN博客](https://blog.csdn.net/m0_46653437/article/details/111443868)
6. [np.argmax()函数用法解析——通俗易懂-CSDN博客](https://blog.csdn.net/weixin_42755982/article/details/104542538)
7. [torch.log — PyTorch 2.5 documentation](https://pytorch.org/docs/stable/generated/torch.log.html)
8. [有效防止softmax计算时上溢出（overflow）和下溢出（underflow）的方法-腾讯云开发者社区-腾讯云](https://cloud.tencent.com/developer/article/1157132)
