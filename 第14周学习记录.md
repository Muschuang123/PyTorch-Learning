## 数学知识

### 偏导数

基本概念上次学过。

#### 有趣的细节：

`f(x, y)`(任意一个一般函数)对不同的变量导相同的次数，可以无视顺序。

<img src="C:\Users\Muschuang123\AppData\Roaming\Typora\typora-user-images\image-20241130122517297.png" alt="image-20241130122517297" style="zoom: 33%;" />

### 梯度 方向导数

一句话概括梯度：函数每个参数对应的偏导数组成的向量。

梯度的几何意义：梯度指向函数图形变化最快的方向。

方向导数：**方向向量**与**梯度**的**点乘积**。表示了在这个方向上函数的导数。

## 自动微分

### 在pytorch中求梯度（gradient）

浮点类型的数才有梯度，因此在定义张量时一定要将类型指定为 float 型

```python
import torch
# 只有当requires_grad参数是True时，pytorch才会对运算过程记录，以便反向传播求解梯度
x = torch.arange(4.0, requires_grad=True)
y = 2 * torch.dot(x, x) 
y.backward()
print(x.grad)
```

这个程序计算的是**▽~x~ 2x^T^x**，结果是**4x**。

默认情况下 pytorch 会累计梯度，所以如果要更新梯度，我们需要使用

```python
x.grad.zero_()
```

来清除之前算出的梯度。

### 反向传播的深层次理解

**读后感：**

反向传播和链式求导法则密切相关。令`requires_grad=True`之后，pytorch会构建一个动态地创建一个**计算图**（一个动态的DAG（底层代码使用C++实现），其中叶子节点是函数的参数，但可能不是同一层函数的参数计算图中的每一个节点，都会封装若干个属性）。使用`tensor.backward()`反向传播时，`backward()`将追溯叶子节点，并将计算出的梯度存入叶子节点的`.grad`属性中。

当`backward()`的使用者是一个标量时，不需要传入任何参数（实际上默认传入的是`torch.tensor(1.0)`）

*当对**向量**或者**矩阵**反向传播时呢？这里我花了很长时间没有理解为什么。不过据说基本用不到`backward()`里面传参的情况，所以我先搁置一下，遇到了再学。*

传进去的参数实际上就是`grad_tensors`，`grad_tensors`的作用其实可以简单地理解成在求梯度时的权重。当我们不需要这个权重时，就可以让这个矩阵求和（或者是点乘同型全1矩阵）。

#### 有趣的细节：

重复调用`backward()`而不更新函数会导致下面的报错：

RuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after **they have already been freed**). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.

黑体部分说明pytorch会在一次反向传播之后自动释放计算图来节省空间，所以需要在多次求梯度的时候应当在`backward()`中加入参数：`retain_graph=True`

### 分离计算

有的时候我们想把某些变量看做常数，这个时候我们可以使用pytorch中的`detach()`来计算。

```python
y = x * x
u = y.detach()
z = u * x
z.sum().backward()
x.grad == u
```

在这段代码中，`z = u * x` 中的u是常数，所以按 z 反向传播得到的x的梯度已经将 u 看做常数，在计算图中，梯度不会流经 u 到 x。

## 总结

经常看到梯度和偏导数在机器学习中至关重要，现在我可以利用pytorch框架自动计算这些内容了，希望以后可以用到。

下周计划：学习概率小节的内容。（期望、方差、条件概率和贝叶斯定理在中学接触过）

<img src="C:\Users\Muschuang123\AppData\Roaming\Typora\typora-user-images\image-20241130134324820.png" alt="image-20241130134324820" style="zoom: 80%;" />

## 参考文献 / 学习资料：

1. [《动手学深度学习》 — 动手学深度学习 2.0.0 documentation](https://zh-v2.d2l.ai/)
2. [多元微积分高清修复版3Blue1Brown创始人Grant Sanderson出品(全69集）](https://www.bilibili.com/video/BV1UB4y1b7ac)
3. [PyTorch：梯度计算之反向传播函数backward()-CSDN博客](https://blog.csdn.net/baidu_38797690/article/details/122180655)
4. [Pytorch autograd,backward详解 - 知乎](https://zhuanlan.zhihu.com/p/83172023)

