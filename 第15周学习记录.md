## 概率

> 概率是一种灵活的语言，用于说明我们的确定程度，并且它可以有效地应用于广泛的领域中。

### 案例：掷骰子

```python
fair_probs = torch.ones([6]) / 6
print(multinomial.Multinomial(1, fair_probs).sample())
```

这个代码中，`fair_probs`中每一个元素的值都是`1/6`，`print`中的函数可以模拟一个掷骰子。

D2L这本书说反复掷骰子并且累加是非常慢的，应当同时抽取多个样本。

<img src="C:\Users\Muschuang123\AppData\Roaming\Typora\typora-user-images\image-20241202173716476.png" alt="image-20241202173716476" style="zoom:80%;" />

为此我专门学了一下python的**时间戳**实验了一下，代码如下：

```python
start = time.time()
cnt = torch.tensor([0, 0, 0, 0, 0, 0])
fair_probs = torch.ones([6]) / 6
for i in range(10000):
    cnt = cnt + multinomial.Multinomial(1, fair_probs).sample()
print(cnt)
end = time.time()
print(end - start)
```

运行结果如下：

```
tensor([1688., 1623., 1703., 1650., 1697., 1639.])
1.551558494567871
```

可见`1e4`的数据这个程序跑了1.62s，确实是很慢的，所以我们应该多层抽样。那么如何多层抽样呢？

~~其实只需要改一个数就好啦：~~

```python
start = time.time()
fair_probs = torch.ones([6]) / 6
print(multinomial.Multinomial(10000, fair_probs).sample())
end = time.time()
print(end - start)
```

运行结果：

```
tensor([1660., 1633., 1698., 1616., 1702., 1691.])
0.0030825138092041016
```

从两次结果来看，每个点数被投掷到的次数在`1666.67`上下，这证明了**大数定律（law of large numbers）**。

### 处理多个随机变量

D2L这本书在之后的内容讲述了许多概念，这些概念并不难，我试着用**一句话概括**：

1. **联合概率**：两个事件同时成立的概率。
2. **条件概率**：一个事件在 另一个事件已经发生时 发生的概率。
3. **贝叶斯定理**：知道 A发生的概率、B发生的概率、B在A发生的条件下发生的概率、A在B条件下发生的概率 中的任何3个，可以求另1个。
4. **边际化**：全概率公式的推广。
5. **独立性**：两个事件独立（无关），记作`A⊥B`。

### [案例：艾滋病检测](https://zh-v2.d2l.ai/chapter_preliminaries/probability.html#subsec-probability-hiv-app)

看了一下，理解了。

## 线性回归

### 概念

先了解一下概念，才能写代码。

> *回归*（regression）是能为一个或多个自变量与因变量之间关系建模的一类方法。 在自然科学和社会科学领域，回归经常用来表示输入和输出之间的关系。

通过阅读，了解了线性回归及其衍生的概念。

我认为：深度学习本质上就是学习已有的数据，并对未来的数据做出预测，这个预测通过线性回归模型来实现。

### 损失函数

> *损失函数*（loss function）能够量化目标的*实际*值与*预测*值之间的差距。 

所以损失函数可以帮助我们调整线性回归模型的参数，让预测值更加精确。

通常选择非负的函数（例如预测值 - 真实值的平方）作为损失函数，函数值为`0`代表完美预测。

<img src="C:\Users\Muschuang123\AppData\Roaming\Typora\typora-user-images\image-20241203205237407.png" alt="image-20241203205237407" style="zoom: 67%;" />

这个函数求和取均值就可以得到损失均值。

### 解析解

> 解析解，是指通过严格的公式所求得的解。给出解的具体函数形式，从解的表达式中就可以算出任何对应值。

线性回归的解可以用一个解析解简单地表达出来。

<img src="C:\Users\Muschuang123\AppData\Roaming\Typora\typora-user-images\image-20241203205956465.png" alt="image-20241203205956465" style="zoom:67%;" />

将上面的解析解代入损失函数，得到`0`，是完美预测。

我动手算了一下产生了个疑问：为什么要引入 **X^T^** 呢？直接用 **X** 计算不好吗？就像这样：

​											**w = X^-1^y**

通过查阅资料，我知道了这个解析解是**对损失函数求导（变量为`w`）**得来的。当导数为`0`时，得到损失函数的极小值。

对损失函数求导后，令导数`= 0`，得 **X^T^Xw = X^T^y**，变换后得上面的解析解，只有方阵才能求逆矩阵，**X^T^X** 保证了矩阵可逆。

不过还有一种情况是因为 **X^T^X **不是满秩导致不可逆，这种情况需要**正则化**来解决。*不过正则化目前D2L这本书还没提到，而且内容比较难，我等遇到了再学吧。*

**！**并不是所有问题的解都有解析解。解析解对问题的限制很严格，导致它无法广泛应用在深度学习里。

### 随机梯度下降

> 本书中我们用到一种名为*梯度下降*（gradient descent）的方法， 这种方法几乎可以优化所有深度学习模型。 它通过不断地在损失函数递减的方向上更新参数来降低误差。

**注意到**其实上次学习的反向传播求出的梯度就是损失函数递减的方向，所以我们可以通过将损失函数的系数沿着梯度微调来优化模型。**更具体地说，**我们需要对 **w** 这个**系数向量**减去 **一个常数 * 梯度**。这个常数`η`叫学习率，另外用于得到梯度的样本批量是`B`。`B`和`η`都是预先设定好的，为了区分他们和 **w** ，这种参数叫做**超参数**。选择超参数的过程就叫**调参**。

<img src="C:\Users\Muschuang123\AppData\Roaming\Typora\typora-user-images\image-20241206202747553.png" alt="image-20241206202747553" style="zoom:67%;" />

> 线性回归恰好是一个在整个域中只有一个最小值的学习问题。 但是对像深度神经网络这样复杂的模型来说，损失平面上通常包含多个最小值。 深度学习实践者很少会去花费大力气寻找这样一组参数，使得在*训练集*上的损失达到最小。 事实上，更难做到的是找到一组参数，这组参数能够在我们从未见过的数据上实现较低的损失， 这一挑战被称为*泛化*（generalization）。

## 矢量化加速

> 在训练我们的模型时，我们经常希望能够同时处理整个小批量的样本。 为了实现这一点，需要我们对计算进行矢量化， 从而利用线性代数库，而不是在Python中编写开销高昂的for循环。

这节其实没啥内容，就是让读者知道，用封装好的库来计算，直接可以矢量相加，提高效率（相比一个元素一个元素的加）。

书中给出了一个封装好的计时器函数~~（发现我在本文开头使用的时间戳太简陋了~~：

```python
class Timer:  #@save
    """记录多次运行时间"""
    def __init__(self):
        self.times = []
        self.start()

    def start(self):
        """启动计时器"""
        self.tik = time.time()

    def stop(self):
        """停止计时器并将时间记录在列表中"""
        self.times.append(time.time() - self.tik)
        return self.times[-1]

    def avg(self):
        """返回平均时间"""
        return sum(self.times) / len(self.times)

    def sum(self):
        """返回时间总和"""
        return sum(self.times)

    def cumsum(self):
        """返回累计时间"""
        return np.array(self.times).cumsum().tolist()
```



## 有趣的细节

用模型预测未知的结果，这一过程被称为***预测*（prediction）**。但业内很多人把它叫做***推断*（inference）**。

> 本书将尝试坚持使用*预测*这个词。 虽然*推断*这个词已经成为深度学习的标准术语，但其实*推断*这个词有些用词不当。 在统计学中，*推断*更多地表示基于数据集估计参数。 当深度学习从业者与统计学家交谈时，术语的误用经常导致一些误解。

## 总结

本周主要学习了**概率**和**线性回归的基本元素**。Python代码写的不多。没事，下周写得多。

下周我将会学习**[3.2. 线性回归的从零开始实现 — 动手学深度学习 2.0.0 documentation](https://zh-v2.d2l.ai/chapter_linear-networks/linear-regression-scratch.html)**。

## 参考文献 / 学习资料

1. [2.6. 概率 — 动手学深度学习 2.0.0 documentation](https://zh-v2.d2l.ai/chapter_preliminaries/probability.html#sec-prob)
2. [联合概率、边际概率、条件概率-CSDN博客](https://blog.csdn.net/libing_zeng/article/details/74625849)
3. [3.1. 线性回归 — 动手学深度学习 2.0.0 documentation](https://zh-v2.d2l.ai/chapter_linear-networks/linear-regression.html)
4. [解析解_百度百科](https://baike.baidu.com/item/解析解/1286740)
5. [线性回归 Linear regression - 知乎](https://zhuanlan.zhihu.com/p/44612139)
6. [线性回归中的正则化 - 知乎](https://zhuanlan.zhihu.com/p/62457875)

