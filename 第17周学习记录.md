### 关于Jupyter Notebook

发现VSCode可以很好的兼容Jupyter Notebook。直接安装插件点击代码就能跑。

## 线性回归的简洁实现

> 在过去的几年里，出于对深度学习强烈的兴趣， 许多公司、学者和业余爱好者开发了各种成熟的开源框架。 这些框架可以自动化基于梯度的学习算法中重复性的工作。 

### 生成并读取数据集

```python
import numpy as np
import torch
from torch.utils import data
from d2l import torch as d2l

true_w = torch.tensor([2, -3.4])
true_b = 4.2
features, labels = d2l.synthetic_data(true_w, true_b, 1000)

def load_array(data_arrays, batch_size, is_train=True):  #@save
    """构造一个PyTorch数据迭代器"""
    dataset = data.TensorDataset(*data_arrays)
    return data.DataLoader(dataset, batch_size, shuffle=is_train)

batch_size = 10
data_iter = load_array((features, labels), batch_size)

print(next(iter(data_iter)))
```

1. `true_w`、`true_b`是用于生成数据的两个参数。`synthetic_data`用于合成数据集（这个函数在上一节手动实现过，这一节直接用D2L这本书提供的。）

2. **`DataLoader`**，负责在模型训练过程中加载和处理数据。<img src="C:\Users\Muschuang123\AppData\Roaming\Typora\typora-user-images\image-20241217153432957.png" alt="image-20241217153432957" style="zoom: 67%;" />

   本段代码用了前三个参数，这里的`shuffle`默认为`False`，所以会在每次运行`print(next(iter(data_iter)))`的时候打乱数据，当我改成`True`之后，每次`print(next(iter(data_iter)))`运行时，输出的数据都是一样的，除非我们重新生成`dataset`。通过`DataLoader`可以用短短几行代码代替上次从零开始实现的代码。

   另外，`DataLoader`可以并行处理数据，使用GPU加速。

   在迭代`DataLoader`时，每次迭代返回的是一个批次的数据。

3. **`TensorDataset`**可以讲数据放进`dataset`中，这个常常与`DataLoader`结合使用。代码段中的`*`与C++中的解引用含义差不多，就是去皮。因为传入参数的时候带着括号。相当于`features`和`labels`合成了一个变量，使用`*`就可以打开这个括号，变回两个变量。

4. `next()`返回迭代器的下一个内容。

### 建立模型并初始化参数

```python
# nn是神经网络的缩写
from torch import nn
net = nn.Sequential(nn.Linear(2, 1))
```

直接用torch中的`nn.Linear(in_features, out_features, bias=True)`就可以建立模型。第三个参数是**是否包含偏置**，其实**偏置**就是是否包含模型里的那个`b`，默认就是有。l另外，我现在知道为啥`w`叫 w 了，其实建立模型的过程其实就是给每个特征标记不同的**权重**。

```python
net[0].weight.data.normal_(0, 0.01)
net[0].bias.data.fill_(0)
```

显然地，这段代码设置了`w`和`b`。`net[0]`代表的`net`的第一层，实际上，我们这个模型只有一层。

### 损失函数

>计算均方误差使用的是`MSELoss`类，也称为平方$L_2$范数。
>默认情况下，它返回所有样本损失的平均值。

```python
loss = nn.MSELoss()
```

这行代码直接就得到损失函数了，真的强。

### 优化算法

```python
trainer = torch.optim.SGD(net.parameters(), lr=0.03)
```

梯度下降。前面的参数其实就是`w`和`b`，然后这个函数能直接从`net`里面取。后面参数是学习率。

### 训练

```python
num_epochs = 3
for epoch in range(num_epochs):
    for X, y in data_iter:
        l = loss(net(X) ,y)
        trainer.zero_grad()
        l.backward()
        trainer.step()
    l = loss(net(features), labels)
    print(f'epoch {epoch + 1}, loss {l:f}')
```

其实这一段与上一节的内容大致相似。训练的过程还是要自己写的。

值得注意的一点是，`trainer`不会在每次训练后自动清除梯度，需要用`optimizer.zero_grad()`，清空梯度。

### 总结

封装好的框架就是好用啊！

## Softmax 回归

与线性回归那一节相同，这一节基本都是概念的讲解，没啥代码量。

**Softmax回归**与其说是回归，更像是一种分类。

### 分类问题

分类问题与回归问题的一个重要的**不同**点：分类问题具有多个输出，而回归问题都是单输出。

多个输出意味着输出是一个向量（一个**概率分布**），那么预测结果其实就是这个向量中的最大值。对于回归问题，我们关心的是一个确切的值（它是否精确）。而对于分类问题来讲，我们关心的是 预测结果在这个向量中的值是否比其他值高出很多，换句话说，我们关心的是正确预测的**置信度**是不是特别大（相较于其他预测结果）。

同时多个输出意味着我们需要有和输出一样多的**仿射函数**。仿射函数就是一个线性的函数。。从这里，可以看到神经网络的雏形。

多个仿射函数和多个输入、输出一起构成了**全连接层**，全连接层的开销特别大。为*`O(输入数量*输出数量)`*。因为每个输入和输出之间都需要一次运算，在之后的学习中，D2L说要给我介绍一种加速方法，可以再把这个*`O`*除以一个超参数，我直接拭目以待。

### softmax运算

> 社会科学家邓肯·卢斯于1959年在*选择模型*（choice model）的理论基础上 发明的*softmax函数*正是这样做的： softmax函数能够将未规范化的预测变换为非负数并且总和为1，同时让模型保持 可导的性质。 为了完成这一目标，我们首先对每个未规范化的预测求幂，这样可以确保输出非负。 为了确保最终输出的概率值总和为1，我们再让每个求幂后的结果除以它们的总和。如下式：
>
> <img src="C:\Users\Muschuang123\AppData\Roaming\Typora\typora-user-images\image-20241221160436630.png" alt="image-20241221160436630" style="zoom:67%;" />

这个公式里的`o`就是刚刚通过仿射函数得到的结果，对这个结果对`e`取指数，`y`非负。除以总和，确保`y.sum() == 1`。

### 损失函数

与线性回归相同的，有了损失函数才能学习，训练模型。

本质上我们是把输入转成向量，然后和**独热标签向量**对比。

<img src="C:\Users\Muschuang123\AppData\Roaming\Typora\typora-user-images\image-20241221185948092.png" alt="image-20241221185948092" style="zoom:67%;" />

结合softmax运算函数，可以得到：

<img src="C:\Users\Muschuang123\AppData\Roaming\Typora\typora-user-images\image-20241221190125039.png" alt="image-20241221190125039" style="zoom:67%;" />



## 参考文献 / 学习资料

1. [3.3. 线性回归的简洁实现 — 动手学深度学习 2.0.0 documentation](https://zh-v2.d2l.ai/chapter_linear-networks/linear-regression-concise.html)
2. [08 线性回归 + 基础优化算法【动手学深度学习v2】](https://www.bilibili.com/video/BV1PX4y1g7KC)
3. [PyTorch入门必学：DataLoader（数据迭代器）参数解析与用法合集_python dataloader-CSDN博客](https://blog.csdn.net/qq_41813454/article/details/134903615)
4. [Pytorch nn.Linear()的基本用法与原理详解及全连接层简介-CSDN博客](https://blog.csdn.net/qq_44722189/article/details/135035351)
5. [PyTorch优化算法：torch.optim.SGD的参数详解和应用-CSDN博客](https://blog.csdn.net/weixin_51659315/article/details/135584143)
6. [3.4. softmax回归 — 动手学深度学习 2.0.0 documentation](https://zh-v2.d2l.ai/chapter_linear-networks/softmax-regression.html)
7. [09 Softmax 回归 + 损失函数 + 图片分类数据集【动手学深度学习v2】](https://www.bilibili.com/video/BV1K64y1Q7wu)
8. [【深度学习 搞笑教程】16 softmax回归 | 草履虫都能听懂 零基础入门 | 持续更新](https://www.bilibili.com/video/BV1Cw411C71D)

### 关于D2L

其实与其说D2L是一本教程，不如说D2L是一本字典，指引我学什么，而不是真正教我学。直接看太难，必须要借助其他的资料。Softmax这一章学的我太卡了，信息论，概率论都没学，直接告诉我一些概念，确实有种被信息轰炸的感觉。所以更多的知识来源就是从各个平台的知识区翻博客，翻视频。

### 下集预告

softmax回归的从零开始实现（像线性回归一样，解读代码）。